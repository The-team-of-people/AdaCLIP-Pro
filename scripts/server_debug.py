import os
import json
import numpy as np
import torch
from modeling.model import AdaCLIP
from modeling.clip import tokenize
from modeling.clip_model import CLIP
from configs.config import parser, parse_with_config

def pad_tokens(tokens, max_txt_len):
    out = torch.zeros((len(tokens), max_txt_len), dtype=torch.long)
    for i, tks in enumerate(tokens):
        if len(tks) > max_txt_len:
            tks = tks[:max_txt_len-1] + [tks[-1]]
        out[i, :len(tks)] = torch.tensor(tks)
    return out

def check_order_and_shape(test_ids, video_ids, video_embs):
    """
    æ£€æŸ¥test_idsï¼ˆå¦‚MSRVTTæµ‹è¯•é›†idé¡ºåºï¼‰ä¸video_ids.jsonæ˜¯å¦ä¸€ä¸€å¯¹åº”å¹¶é¡ºåºä¸€è‡´ï¼ŒåŒæ—¶æ£€æŸ¥embs shape
    """
    if not test_ids or len(test_ids) == 0:
        print("âš ï¸ æœªæŒ‡å®štest_idsï¼Œè·³è¿‡é¡ºåºæ£€æŸ¥ã€‚")
        return

    print(f"æ£€æŸ¥ test_ids({len(test_ids)})ï¼Œvideo_ids({len(video_ids)})ï¼Œvideo_embs({len(video_embs)})...")

    # æ£€æŸ¥ test_ids æ˜¯å¦éƒ½åœ¨ video_ids
    missing = [vid for vid in test_ids if vid not in video_ids]
    if missing:
        print(f"âŒ ä»¥ä¸‹test set idåœ¨video_ids.jsonä¸­æ‰¾ä¸åˆ°: {missing}")
    else:
        print("âœ… æ‰€æœ‰test set idéƒ½åŒ…å«åœ¨video_ids.jsonä¸­ã€‚")

    # æ£€æŸ¥æ•°é‡
    if len(test_ids) > len(video_ids):
        print(f"âŒ test_idsæ•°é‡({len(test_ids)})å¤§äºvideo_ids({len(video_ids)})ï¼Œè¯·æ£€æŸ¥ï¼")

    if len(video_embs) != len(video_ids):
        print(f"âŒ video_embsæ•°é‡({len(video_embs)})ä¸video_idsæ•°é‡({len(video_ids)})ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ï¼")
    else:
        print("âœ… video_embsæ•°é‡ä¸video_idsæ•°é‡ä¸€è‡´ã€‚")

    # æ£€æŸ¥é¡ºåº
    all_match = True
    min_len = min(len(test_ids), len(video_ids))
    for idx in range(min_len):
        if video_ids[idx] != test_ids[idx]:
            print(f"âŒ é¡ºåºä¸ä¸€è‡´: test_ids[{idx}]={test_ids[idx]}, video_ids[{idx}]={video_ids[idx]}")
            all_match = False
    if all_match and len(test_ids) == len(video_ids):
        print("âœ… test set idé¡ºåºå’Œvideo_ids.jsonå®Œå…¨ä¸€è‡´ï¼")
    elif all_match:
        print("âš ï¸ test_idså’Œvideo_idsé¡ºåºä¸€è‡´ï¼Œä½†é•¿åº¦ä¸åŒã€‚")
    else:
        print("âŒ test set idé¡ºåºå’Œvideo_ids.jsonæœ‰ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥ï¼")
    return all_match

def search(model, cfg, video_embs, video_ids, text, device, subset_ids=None):
    tokens = tokenize([text], context_length=cfg.max_txt_len)
    tokens = pad_tokens(tokens, cfg.max_txt_len)
    if tokens.dim() == 2:
        tokens = tokens.unsqueeze(1)  # [batch, 1, seq_len]
    tokens = tokens.to(device)
    with torch.no_grad():
        text_embd, _ = model.get_text_output(tokens, return_hidden=False)  # (1, 1, D)
        text_embd = text_embd.squeeze(0).squeeze(0)
        text_embd = text_embd / text_embd.norm(dim=-1, keepdim=True)
    # è§†é¢‘ç¼–ç 
    if subset_ids is not None:
        indices = [video_ids.index(vid) for vid in subset_ids if vid in video_ids]
        sub_video_embs = video_embs[indices]
        sub_video_ids = [video_ids[i] for i in indices]
    else:
        sub_video_embs = video_embs
        sub_video_ids = video_ids
    # è½¬ä¸º torchï¼Œæ¬åˆ° device
    video_embs_tensor = torch.tensor(sub_video_embs, dtype=torch.float32, device=device)
    video_embs_tensor = video_embs_tensor / video_embs_tensor.norm(dim=-1, keepdim=True)
    # æ‰¹é‡ç›¸ä¼¼åº¦
    sims = torch.matmul(video_embs_tensor, text_embd)
    sims = sims.cpu().numpy()
    sorted_idx = np.argsort(-sims)
    results = [(sub_video_ids[i], float(sims[i])) for i in sorted_idx]
    return results, sims, sub_video_ids

if __name__ == "__main__":
    import argparse
    parser_arg = argparse.ArgumentParser()
    parser_arg.add_argument("--config", type=str, default="configs/msrvtt-jsfusion.json")
    parser_arg.add_argument("--video_embs", type=str, default="data/application/video_embs.npy")
    parser_arg.add_argument("--video_ids", type=str, default="data/application/video_ids.json")
    parser_arg.add_argument("--device", type=str, default="cuda")
    parser_arg.add_argument("--frames_dir", type=str, default="data/MSRVTT/frames")
    parser_arg.add_argument("--compare_json", type=str, default="data/application/server_compare_list.json")
    parser_arg.add_argument("--processed_json", type=str, default="data/application/processed_videos.json")
    parser_arg.add_argument("--test_ids", type=str, default="data/MSRVTT/test_video_ids.txt", help="æµ‹è¯•é›†idé¡ºåºæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªvideo idï¼Œå¯ä¸ºç©ºï¼‰")
    args = parser_arg.parse_args()

    parsed_args = parser.parse_args(args=[])
    parsed_args.config = args.config
    cfg = parse_with_config(parsed_args)
    cfg.frames_dir = args.frames_dir

    # è‡ªåŠ¨åˆ‡æ¢CPU/GPU
    device = torch.device(args.device if torch.cuda.is_available() and args.device.startswith("cuda") else "cpu")

    # åŠ è½½å…¨éƒ¨è§†é¢‘ç‰¹å¾å’ŒID
    video_embs = np.load(args.video_embs)
    with open(args.video_ids, "r") as f:
        video_ids = json.load(f)

    # åŠ è½½test_ids
    test_ids = []
    if args.test_ids and os.path.isfile(args.test_ids):
        with open(args.test_ids, "r") as f:
            test_ids = [line.strip() for line in f if line.strip()]

    # æ£€æŸ¥é¡ºåºå’Œshape
    check_order_and_shape(test_ids, video_ids, video_embs)

    # ä¼˜å…ˆç”¨ compare_jsonï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨ processed_jsonï¼ˆå…¨é‡è§†é¢‘ï¼‰
    if os.path.isfile(args.compare_json):
        with open(args.compare_json, "r") as f:
            compare_ids = json.load(f)
            print(f"Loaded {len(compare_ids)} video ids for comparison from {args.compare_json}")
    elif os.path.isfile(args.processed_json):
        with open(args.processed_json, "r") as f:
            compare_ids = json.load(f)
            print(f"Loaded {len(compare_ids)} video ids for comparison from {args.processed_json}")
    else:
        compare_ids = None
        print("No compare_json or processed_json found, will use all video embeddings.")

    # åŠ è½½æ¨¡å‹
    clip_state_dict = CLIP.get_config(pretrained_clip_name=cfg.clip_backbone)
    model = AdaCLIP(cfg, clip_state_dict)
    ckpt_path = os.path.join("checkpoints", cfg.dataset, "trained_model.pth")
    if os.path.isfile(ckpt_path):
        checkpoint = torch.load(ckpt_path, map_location="cpu")
        if 'model' in checkpoint:
            model.load_state_dict(checkpoint['model'].state_dict(), strict=False)
        else:
            model.load_state_dict(checkpoint, strict=False)
    model = model.to(device)
    if device.type == "cpu":
        model = model.float()
    model.eval()

    print(f"Ready for text-to-video retrieval! (device: {device})")
    print("å¦‚éœ€é¡ºåºdebugï¼Œè¯·ç¡®ä¿--test_idsä¸ºæµ‹è¯•é›†idé¡ºåºæ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªvideo idã€‚")
    # å½“å‰ç”¨äºè‡ªåŠ¨ç´¢å¼•çš„queryåºå·
    query_idx = 0
    while True:
        try:
            query = input("è¯·è¾“å…¥æ–‡æœ¬æŸ¥è¯¢ï¼ˆå›è½¦é€€å‡ºï¼‰ï¼š").strip()
        except (EOFError, KeyboardInterrupt):
            print("\né€€å‡ºæ£€ç´¢ã€‚")
            break
        if not query:
            break

        subset_ids = compare_ids  # compare_ids å¯èƒ½ä¸º Noneï¼Œè¡¨ç¤ºå…¨é‡å¯¹æ¯”

        results, sims, sub_video_ids = search(model, cfg, video_embs, video_ids, query, device, subset_ids)

        print("å…¨éƒ¨æ’åºåçš„è§†é¢‘IDåŠç›¸ä¼¼åº¦ï¼š")
        for rank, (vid, score) in enumerate(results[:50], 1):
            print(f"{rank}. {vid}\tScore: {score:.4f}")

        # å¦‚æœtest_idså­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œè¾“å‡ºground-truthæ’å
        if test_ids and len(test_ids) > query_idx:
            gt_vid = test_ids[query_idx]
            gt_rank = None
            for i, (vid, score) in enumerate(results):
                if vid == gt_vid:
                    gt_rank = i+1
                    print(f"ğŸ‘‰ Ground-truthè§†é¢‘id: {gt_vid} åœ¨å½“å‰queryä¸‹æ’å: {gt_rank}, ç›¸ä¼¼åº¦: {score:.4f}")
                    break
            if gt_rank is None:
                print(f"âš ï¸ Ground-truthè§†é¢‘id: {gt_vid} æœªåœ¨compare_idsæˆ–æ£€ç´¢ç»“æœä¸­ï¼")
            query_idx += 1
        else:
            print("æœªæŒ‡å®šground-truthæˆ–æŸ¥è¯¢åºå·è¶…å‡ºtest_idsèŒƒå›´ã€‚")